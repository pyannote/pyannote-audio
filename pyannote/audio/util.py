#!/usr/bin/env python
# encoding: utf-8

# The MIT License (MIT)

# Copyright (c) 2016-2018 CNRS

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# AUTHORS
# HervÃ© BREDIN - http://herve.niderb.fr

import os
import errno
import collections
import numpy as np
from pyannote.core import Segment
from pyannote.core import Annotation
from pyannote.core import SlidingWindowFeature
from pyannote.core.util import string_generator
from pyannote.database import get_label_identifier


def mkdir_p(path):
    """Create directory and all its parents if they do not exist

    This is the equivalent of Unix 'mkdir -p path'

    Parameter
    ---------
    path : str
        Path to new directory.

    Reference
    ---------
    http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python
    """

    try:
        os.makedirs(path)
    except OSError as exc:  # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise exc


def to_numpy(current_file, precomputed, labels=None):
    """Convert annotation to numpy array

    Parameters
    ----------
    current_file : dict
    precomputed : Precomputed
    labels : list, optional
        Predefined list of labels.  Defaults to using `annotation` labels.

    Returns
    -------
    y : numpy.ndarray
        (N, K) array where y[t, k] > 0 when labels[k] is active at timestep t.
    labels : list
        List of labels.

    See also
    --------
    See `from_numpy` to convert `y` back to a pyannote.core.Annotation instance
    """

    annotation = current_file['annotation']

    if labels is None:
        labels = [get_label_identifier(label, current_file)
                  for label in annotation.labels()]
    indices = {label: i for i, label in enumerate(labels)}

    # number of samples
    N, _ = precomputed.shape(current_file)
    # number of classes
    K = len(labels)
    # one-hot encoding
    y = np.zeros((N, K), dtype=np.uint8)

    sw = precomputed.sliding_window()
    for segment, _, label in annotation.itertracks(yield_label=True):
        label = get_label_identifier(label, current_file)
        try:
            k = indices[label]
        except KeyError as e:
            raise

        for i, j in sw.crop(segment, mode='center', return_ranges=True):
            i = max(0, i)
            j = min(N, j)
            y[i:j, k] += 1

    return y, labels


def from_numpy(y, precomputed, labels=None):
    """Convert numpy array to annotation

    Parameters
    ----------
    y : (N, K) or (N, ) numpy.ndarray
        When y has shape (N, K), y[t, k] > 0 means kth label is active at
        timestep t. When y has shape (N, ), y[t] = 0 means no label is active
        at timestep t, y[t] = k means (k-1)th label is active.
    precomputed : Precomputed
    labels : list, optional
        Predefined list of labels.  Defaults to labels generated by
        `pyannote.core.utils.string_generator`.

    Returns
    -------
    annotation : pyannote.core.Annotation

    See also
    --------
    `to_numpy`
    """

    window = precomputed.sliding_window()

    if len(y.shape) < 2:
        N, = y.shape
        if labels is not None:
            K = len(labels)
        else:
            K = np.max(y)

        y_ = np.zeros((N, K), dtype=np.int8)
        for t, k in enumerate(y):
            if k == 0:
                continue
            y_[t, k - 1] = 1
        y = y_

    N, K = y.shape

    if labels is None:
        labels = string_generator()
        labels = [next(labels) for _ in range(K)]

    annotation = Annotation()

    y_off = np.zeros((1, K), dtype=np.int8)
    y = np.vstack((y_off, y, y_off))
    diff = np.diff(y, axis=0)
    for k, label in enumerate(labels):
        for t in np.where(diff[:, k] != 0)[0]:
            if diff[t, k] > 0:
                onset_t = window[t].middle
            else:
                segment = Segment(onset_t, window[t].middle)
                annotation[segment, k] = label

    return annotation


class DavisKingScheduler(object):
    """Automatic Learning Rate Scheduling That Really Works

    http://blog.dlib.net/2018/02/automatic-learning-rate-scheduling-that.html

    Parameters
    ----------
    optimizer : Optimizer
        Wrapped optimizer.
    factor : float, optional
        Factor by which the learning rate will be reduced.
        new_lr = old_lr * factor. Defaults to 0.5
    patience : int, optional
        Number of batches with no improvement after which learning rate will
        be reduced. Defaults to 1000.
    active : bool, optional
        Set to False to not update learning rate.

    Usage
    -----
    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
    >>> scheduler = DavisKingScheduler(optimizer)
    >>> for mini_batch in batches:
    ...     mini_loss = train(mini_batch, optimizer)
    ...     scheduler.step(mini_loss)
    """

    def __init__(self, optimizer, factor=0.5, patience=1000, active=True):

        super(DavisKingScheduler, self).__init__()

        if factor >= 1.0:
            raise ValueError('Factor should be < 1.0.')
        self.factor = factor

        self.optimizer = optimizer
        self.patience = patience
        self.active = active

        self.lr_ = [float(grp['lr']) for grp in self.optimizer.param_groups]
        self.losses_ = collections.deque([], maxlen=self.patience + 1)

    @property
    def lr(self):
        return tuple(self.lr_)

    def step(self, loss):

        from dlib import count_steps_without_decrease
        from dlib import count_steps_without_decrease_robust
        from dlib import probability_that_sequence_is_increasing

        self.losses_.append(loss)

        count = count_steps_without_decrease(self.losses_)
        count_robust = count_steps_without_decrease_robust(self.losses_)

        if len(self.losses_) > 2:
            increasing = probability_that_sequence_is_increasing(self.losses_)
        else:
            increasing = np.NAN

        if self.active and count > self.patience and count_robust > self.patience:
            self._reduce_lr()
            self.losses_.clear()

        return {'steps_without_decrease': count / self.patience,
                'steps_without_decrease_robust': count_robust / self.patience,
                'increasing_probability': increasing}

    def _reduce_lr(self):
        self.lr_ = []
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group['lr'])
            new_lr = old_lr * self.factor
            param_group['lr'] = new_lr
            self.lr_.append(new_lr)
