# @package _group_

run:
  # dir: train/${now:%Y-%m-%d}/${now:%H-%M-%S}
  dir: ${protocol}/${task._target_}/${now:%Y-%m-%d}/${now:%H-%M-%S}

sweep:
  dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}/${protocol}/${task._target_}
  subdir: ${hydra.job.num}

output_subdir: ""

help:
  app_name: pyannote-audio-train

  # Help header, customize to describe your app to your users
  header: == ${hydra.help.app_name} ==

  footer: |-
    Powered by Hydra (https://hydra.cc)
    Use --hydra-help to view Hydra specific help

  # Basic Hydra flags:
  #   $FLAGS_HELP
  #
  # Config groups, choose one of:
  #   $APP_CONFIG_GROUPS: All config groups that does not start with hydra/.
  #   $HYDRA_CONFIG_GROUPS: All the Hydra config groups (starts with hydra/)
  #
  # Configuration generated with overrides:
  #   $CONFIG : Generated config
  #
  template: |-
    ${hydra.help.header}

    pyannote-audio-train protocol={protocol_name} task={task} model={model}

    {task} can be any of the following:
    * vad (default) = voice activity detection
    * scd = speaker change detection
    * osd = overlapped speech detection
    * xseg = multi-task segmentation

    {model} can be any of the following:
    * debug (default) = simple segmentation model for debugging purposes

    {optimizer} can be any of the following
    * adam (default) = Adam optimizer

    {trainer}Â can be any of the following
    * fast_dev_run for debugging
    * default (default) for training the model

    Options
    =======

    Here, we describe the most common options: use "--cfg job" option to get a complete list.

    * task.duration: audio chunk duration (in seconds)
    * task.batch_size: number of audio chunks per batch
    * task.num_workers: number of workers used for generating training chunks

    * optimizer.lr: learning rate
    * trainer.auto_lr_find: use pytorch-lightning AutoLR

    Grid search
    ===========

    Because it is powered by Hydra, one can run grid search using the --multirun option.

    For instance, the following command will run the same job three times, with three different learning rates:
      pyannote-audio-train --multirun protocol={protocol_name} task={task} optimizer.lr=1e-3,1e-2,1e-1

    This one will train two models: one for voice activity detection and one for speaker change detection.
      pyannote-audio-train --multirun protocol={protocol_name} task=vad,scd,osd

    ${hydra.help.footer}
