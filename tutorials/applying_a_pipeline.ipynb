{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying a pretrained pipeline\n",
    "\n",
    "In this tutorial, you will learn how to apply `pyannote.audio` pipelines on an audio file.\n",
    "\n",
    "A pipeline takes an audio file as input and returns a labeled temporal segmentation of the audio file. \n",
    "\n",
    "More precisely, it usually applies a pretrained model (= neural network) on the audio file, post-processes the output of the model, and returns its output as a [`pyannote.core.Annotation`](http://pyannote.github.io/pyannote-core/structure.html#annotation) instance. It should become clearer as you keep reading..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pipeline from ðŸ¤— hub\n",
    "\n",
    "Pretrained pipelines are available on [ðŸ¤— Huggingface model hub](https://hf.co/models?other=pyannote-audio-pipeline) and can be listed by looking for the [`pyannote-audio-pipeline`](https://hf.co/models?other=pyannote-audio-pipeline) tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "available_pipelines = [p.modelId for p in HfApi().list_models(filter=\"pyannote-audio-pipeline\")]\n",
    "available_pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the speaker diarization pipeline..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a file from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and apply it to an audio file.  \n",
    "\n",
    "The pipeline will automatically use GPUs when available. \n",
    "On CPU it might take a long while (up to 10x RT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/Users/bredin/Development/pyannote/pyannote-audio\"\n",
    "AUDIO_FILE = f\"{ROOT_DIR}/tutorials/assets/sample.wav\"\n",
    "dia = pipeline(AUDIO_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the output\n",
    "\n",
    "Most pipelines return a [`pyannote.core.Annotation`](http://pyannote.github.io/pyannote-core/structure.html#annotation) instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation\n",
    "assert isinstance(dia, Annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... whose [API](https://pyannote.github.io/pyannote-core/structure.html#annotation) you can use to print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for speech_turn, track, speaker in dia.itertracks(yield_label=True):\n",
    "    print(f\"{speech_turn.start:4.1f} {speech_turn.end:4.1f} {speaker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you happen to be running this example in a _Jupyter notebook_, `dia` can be [visualized directly](http://pyannote.github.io/pyannote-core/visualization.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we visualize [0, 30] time range\n",
    "from pyannote.core import notebook, Segment\n",
    "notebook.crop = Segment(0, 30)\n",
    "dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When available, the reference annotation can be visualized too, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database.util import load_rttm\n",
    "REFERENCE = f\"{ROOT_DIR}/tutorials/assets/sample.rttm\"\n",
    "reference = load_rttm(REFERENCE)[\"sample\"]\n",
    "\n",
    "# map hypothesized and reference speakers for visualization purposes\n",
    "pipeline.optimal_mapping(dia, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a file from memory\n",
    "\n",
    "In case the audio file is not stored on disk, pipelines can also process audio provided as a `{\"waveform\": ..., \"sample_rate\": ...}` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_FILE)\n",
    "\n",
    "print(f\"{type(waveform)=}\")\n",
    "print(f\"{waveform.shape=}\")\n",
    "print(f\"{waveform.dtype=}\")\n",
    "\n",
    "audio_in_memory = {\"waveform\": waveform, \"sample_rate\": sample_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\")\n",
    "vad(audio_in_memory)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41379f2c2a4eb17f5ac9a1f5014f4b793a0ead0b6469d8877f81a91eb030f53e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('pyannote': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
