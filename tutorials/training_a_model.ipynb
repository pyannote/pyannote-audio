{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (or fine-tuning) a model\n",
    "\n",
    "In this tutorial, you will learn how to train a `pyannote.audio` model from scratch, or fine-tune a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** this tutorial assumes that the [AMI corpus](https://groups.inf.ed.ac.uk/ami/corpus/) has already been [setup for use with `pyannote`](https://github.com/pyannote/AMI-diarization-setup/tree/main/pyannote) and the `PYANNOTE_DATABASE_CONFIG` environment variable is set accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining which `task` the `model` will address.  \n",
    "Here, we want the `model` to address voice activity detection (`vad`) using the `ami` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import get_protocol\n",
    "ami = get_protocol('AMI.SpeakerDiarization.only_words')\n",
    "\n",
    "from pyannote.audio.tasks import VoiceActivityDetection\n",
    "vad = VoiceActivityDetection(ami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we define a `compute_model_fscore` function that runs a model on the AMI test set and returns the voice activity detection F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import VoiceActivityDetection as VoiceActivityDetectionPipeline\n",
    "from pyannote.metrics.detection import DetectionPrecisionRecallFMeasure\n",
    "\n",
    "def compute_model_fscore(model):\n",
    "\n",
    "    # instantiate voice activity detection pipeline\n",
    "    vad = VoiceActivityDetectionPipeline(segmentation=model)\n",
    "    vad.instantiate({'onset': 0.5, 'offset': 0.5, \n",
    "                     'min_duration_on': 0.0, 'min_duration_off': 0.0})\n",
    "\n",
    "    # instantiate precision/recall metrics\n",
    "    metric = DetectionPrecisionRecallFMeasure()\n",
    "\n",
    "    for file in ami.test():\n",
    "        \n",
    "        # apply the voice activity detection pipeline\n",
    "        speech = vad(file)\n",
    "        \n",
    "        # evaluate its output\n",
    "        _ = metric(\n",
    "            file['annotation'],     # this is the reference annotation\n",
    "            speech,                 # this is the hypothesized annotation\n",
    "            uem=file['annotated'])  # this is the part of the file that should be evaluated\n",
    "\n",
    "    # aggregate the performance over the whole test set\n",
    "    fscore = abs(metric)\n",
    "    print(f'F-score = {100 * fscore:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve as a baseline, we use the pretrained [`pyannote/segmentation`](https://hf.co/pyannote/segmentation) speaker segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "pretrained = Model.from_pretrained('pyannote/segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pretrained` model relies on the `PyanNet` architecture available in `pyannote.audio`, that combines (trainable) SincNet feature extraction, a few LSTM layers, a few linear layers and a final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a voice activity detection model from scratch, using the AMI training set.\n",
    "\n",
    "To make sure we use the exact same architecture, we rely on `pretrained.hparams` that conveniently keeps track of the hyper-parameters used to instantiate the architecture of `pretrained` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "from_scratch = PyanNet(task=vad, **pretrained.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ  Notice how we passed `vad` as the `task` argument of our `from_scratch` model.  \n",
    "This allows `pyannote.audio` to automagically register the right `classifier` and `activation` layers into the `PyanNet` model.\n",
    "\n",
    "> Look ma, no hands!\n",
    "\n",
    "This magic trick is possible because every task in `pyannote.audio` exposes its specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad.specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activity detection is a *binary classification* problem that is trained on *2s* audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=2)\n",
    "trainer.fit(from_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(from_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§∑‚Äç‚ôÇÔ∏èTraining the model for just 2 epochs gives us decent results but it still performs worse than the pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained model\n",
    "\n",
    "ü§î Can we do better (and faster) by fine-tuning the pretrained model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = Model.from_pretrained('pyannote/segmentation')\n",
    "fine_tuned.task = vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=1)\n",
    "trainer.fit(fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(fine_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ Fine-tuning the pretrained model for just one epoch already gives us an improvement (96.8%) over the pretrained model (96.6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "Data augmentation is supported via [`torch-audiomentations`](https://github.com/asteroid-team/torch-audiomentations).\n",
    "\n",
    "```python\n",
    "from torch_audiomentations import Compose, ApplyImpulseResponse, AddBackgroundNoise\n",
    "augmentation = Compose(transforms=[ApplyImpulseResponse(...),\n",
    "                                   AddBackgroundNoise(...)])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "A growing collection of tasks can be addressed.\n",
    "*Here, we address speaker segmentation.*\n",
    "\n",
    "```python\n",
    "from pyannote.audio.tasks import Segmentation\n",
    "seg = Segmentation(ami, augmentation=augmentation)\n",
    "```\n",
    "\n",
    "A growing collection of model architecture can be used.\n",
    "*Here, we use the PyanNet (sincnet + LSTM) architecture.*\n",
    "\n",
    "```python\n",
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "model = PyanNet(task=seg)\n",
    "```\n",
    "\n",
    "We benefit from all the nice things that [`pytorch-lightning`](https://www.pytorchlightning.ai/) has to offer:  distributed (GPU & TPU) training, model checkpointing, logging, etc.\n",
    "*In this example, we don't really use any of this...*\n",
    "\n",
    "```python\n",
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)\n",
    "```\n",
    "\n",
    "Predictions are obtained by wrapping the model into the `Inference` engine.\n",
    "\n",
    "```python\n",
    "from pyannote.audio import Inference\n",
    "inference = Inference(model)\n",
    "predictions = inference('audio.wav')\n",
    "```\n",
    "\n",
    "Pretrained models can be shared on [Huggingface.co](https://huggingface.co/pyannote) model hub.\n",
    "*Here, we download and use a [pretrained](https://huggingface.co/pyannote/segmentation) segmentation model.*\n",
    "\n",
    "```python\n",
    "inference = Inference('pyannote/segmentation')\n",
    "predictions = inference('audio.wav')\n",
    "```\n",
    "\n",
    "Fine-tuning is as easy as setting the `task` attribute, freezing early layers and training.\n",
    "*Here, we fine-tune on AMI dataset the pretrained segmentation model.*\n",
    "\n",
    "```python\n",
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained('pyannote/segmentation')\n",
    "model.task = Segmentation(ami)\n",
    "model.freeze_up_to('sincnet')\n",
    "trainer.fit(model)\n",
    "```\n",
    "\n",
    "Transfer learning is also supported out of the box.\n",
    "*Here, we do transfer learning from segmentation to overlapped speech detection.*\n",
    "\n",
    "```python\n",
    "from pyannote.audio.tasks import OverlappedSpeechDetection\n",
    "osd = OverlappedSpeechDetection(ami)\n",
    "model.task = osd\n",
    "trainer.fit(model)\n",
    "```\n",
    "\n",
    "Default optimizer (`Adam` with default parameters) is automatically set up for you.  Customizing optimizer (and scheduler) requires overriding [`model.configure_optimizers`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers) method:\n",
    "\n",
    "```python\n",
    "from types import MethodType\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "def configure_optimizers(self):\n",
    "    return {\"optimizer\": SGD(self.parameters()),\n",
    "            \"lr_scheduler\": ExponentialLR(optimizer, 0.9)}\n",
    "model.configure_optimizers = MethodType(configure_optimizers, model)\n",
    "trainer.fit(model)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
