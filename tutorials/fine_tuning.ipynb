{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (or fine-tuning) a model\n",
    "\n",
    "In this tutorial, you will learn how to train a `pyannote.audio` model from scratch, or fine-tune a pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** this tutorial assumes that the [AMI corpus](https://groups.inf.ed.ac.uk/ami/corpus/) has already been [setup for use with `pyannote`](https://github.com/pyannote/AMI-diarization-setup/tree/main/pyannote) and the `PYANNOTE_DATABASE_CONFIG` environment variable is set accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining which `task` the `model` will address.  \n",
    "Here, we want the `model` to address voice activity detection (`vad`) using the `ami` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import get_protocol\n",
    "ami = get_protocol('AMI.SpeakerDiarization.only_words')\n",
    "\n",
    "from pyannote.audio.tasks import VoiceActivityDetection\n",
    "vad = VoiceActivityDetection(ami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we define a `compute_model_fscore` function that runs a model on the AMI test set and returns the voice activity detection F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import VoiceActivityDetection as VoiceActivityDetectionPipeline\n",
    "from pyannote.metrics.detection import DetectionPrecisionRecallFMeasure\n",
    "\n",
    "def compute_model_fscore(model):\n",
    "\n",
    "    # instantiate voice activity detection pipeline\n",
    "    vad = VoiceActivityDetectionPipeline(segmentation=model)\n",
    "    vad.instantiate({'onset': 0.5, 'offset': 0.5, \n",
    "                     'min_duration_on': 0.0, 'min_duration_off': 0.0})\n",
    "\n",
    "    # instantiate precision/recall metrics\n",
    "    metric = DetectionPrecisionRecallFMeasure()\n",
    "\n",
    "    for file in ami.test():\n",
    "        \n",
    "        # apply the voice activity detection pipeline\n",
    "        speech = vad(file)\n",
    "        \n",
    "        # evaluate its output\n",
    "        _ = metric(\n",
    "            file['annotation'],     # this is the reference annotation\n",
    "            speech,                 # this is the hypothesized annotation\n",
    "            uem=file['annotated'])  # this is the part of the file that should be evaluated\n",
    "\n",
    "    # aggregate the performance over the whole test set\n",
    "    fscore = abs(metric)\n",
    "    print(f'F-score = {100 * fscore:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve as a baseline, we use the pretrained [`pyannote/segmentation`](https://hf.co/pyannote/segmentation) speaker segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "pretrained = Model.from_pretrained('pyannote/segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pretrained` model relies on the `PyanNet` architecture available in `pyannote.audio`, that combines (trainable) SincNet feature extraction, a few LSTM layers, a few linear layers and a final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name       | Type       | Params | In sizes      | Out sizes                                  \n",
       "--------------------------------------------------------------------------------------------------------\n",
       "0 | sincnet    | SincNet    | 42.6 K | [3, 1, 32000] | [3, 60, 115]                               \n",
       "1 | lstm       | LSTM       | 1.4 M  | [3, 115, 60]  | [[3, 115, 256], [[8, 3, 128], [8, 3, 128]]]\n",
       "2 | linear     | ModuleList | 49.4 K | ?             | ?                                          \n",
       "3 | classifier | Linear     | 516    | [3, 115, 128] | [3, 115, 4]                                \n",
       "4 | activation | Sigmoid    | 0      | [3, 115, 4]   | [3, 115, 4]                                \n",
       "--------------------------------------------------------------------------------------------------------\n",
       "1.5 M     Trainable params\n",
       "0         Non-trainable params\n",
       "1.5 M     Total params\n",
       "5.892     Total estimated model params size (MB)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score = 96.6%\n"
     ]
    }
   ],
   "source": [
    "compute_model_fscore(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a voice activity detection model from scratch, using the AMI training set.\n",
    "\n",
    "To make sure we use the exact same architecture, we rely on `pretrained.hparams` that conveniently keeps track of the hyper-parameters used to instantiate the architecture of `pretrained` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"linear\":       {'hidden_size': 128, 'num_layers': 2}\n",
       "\"lstm\":         {'hidden_size': 128, 'num_layers': 4, 'bidirectional': True, 'monolithic': True, 'dropout': 0.5, 'batch_first': True}\n",
       "\"num_channels\": 1\n",
       "\"sample_rate\":  16000\n",
       "\"sincnet\":      {'stride': 10, 'sample_rate': 16000}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "from_scratch = PyanNet(task=vad, **pretrained.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👀  Notice how we passed `vad` as the `task` argument of our `from_scratch` model.  \n",
    "This allows `pyannote.audio` to automagically register the right `classifier` and `activation` layers into the `PyanNet` model.\n",
    "\n",
    "> Look ma, no hands!\n",
    "\n",
    "This magic trick is possible because every task in `pyannote.audio` exposes its specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Specifications(problem=<Problem.BINARY_CLASSIFICATION: 0>, resolution=<Resolution.FRAME: 1>, duration=2.0, warm_up=(0.0, 0.0), classes=['speech'], permutation_invariant=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad.specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activity detection is a *binary classification* problem that is trained on *2s* audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type       | Params | In sizes       | Out sizes                                     \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "0 | sincnet           | SincNet    | 42.6 K | [32, 1, 32000] | [32, 60, 115]                                 \n",
      "1 | lstm              | LSTM       | 1.4 M  | [32, 115, 60]  | [[32, 115, 256], [[8, 32, 128], [8, 32, 128]]]\n",
      "2 | linear            | ModuleList | 49.4 K | ?              | ?                                             \n",
      "3 | classifier        | Linear     | 129    | [32, 115, 128] | [32, 115, 1]                                  \n",
      "4 | activation        | Sigmoid    | 0      | [32, 115, 1]   | [32, 115, 1]                                  \n",
      "5 | validation_metric | AUROC      | 0      | ?              | ?                                             \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.890     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1199620bde304528aa05944a6cffae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=2)\n",
    "trainer.fit(from_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score = 96.4%\n"
     ]
    }
   ],
   "source": [
    "compute_model_fscore(from_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤷‍♂️Training the model for just 2 epochs gives us decent results but it still performs worse than the pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning a pretrained model\n",
    "\n",
    "🤔 Can we do better (and faster) by fine-tuning the pretrained model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = Model.from_pretrained('pyannote/segmentation')\n",
    "fine_tuned.task = vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type       | Params | In sizes       | Out sizes                                     \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "0 | sincnet           | SincNet    | 42.6 K | [32, 1, 32000] | [32, 60, 115]                                 \n",
      "1 | lstm              | LSTM       | 1.4 M  | [32, 115, 60]  | [[32, 115, 256], [[8, 32, 128], [8, 32, 128]]]\n",
      "2 | linear            | ModuleList | 49.4 K | ?              | ?                                             \n",
      "3 | classifier        | Linear     | 129    | [32, 115, 128] | [32, 115, 1]                                  \n",
      "4 | activation        | Sigmoid    | 0      | [32, 115, 1]   | [32, 115, 1]                                  \n",
      "5 | validation_metric | AUROC      | 0      | ?              | ?                                             \n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.890     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d432841a7f1403c97134157dc8b4c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=1)\n",
    "trainer.fit(fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score = 96.8%\n"
     ]
    }
   ],
   "source": [
    "compute_model_fscore(fine_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉 Fine-tuning the pretrained model for just one epoch already gives us an improvement (96.8%) over the pretrained model (96.6%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
